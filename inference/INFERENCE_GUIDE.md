# Stock Selection Model - Inference Guide

## Overview
This guide provides precise instructions for using the trained TransformerStockPrediction model for stock selection inference. The model predicts normalized stock returns and selects top-performing stocks for portfolio construction.

## Quick Start

### Prerequisites
1. **Trained model checkpoint** (`.ckpt` file)
2. **Input data** in NASDAQ_all_features.pkl format
3. **Python environment** with PyTorch, NumPy, and dependencies

### Basic Usage
```bash
python stock_inference.py --model_path "./models/model_tt_50.ckpt" --data_path "./data/NASDAQ_all_features.pkl"
```

## Data Input Requirements

### Expected Data Format

The model expects input data in a specific format. Here are the precise requirements:

#### 1. Pickle File Format (NASDAQ_all_features.pkl)
```python
data = {
    'all_features': {
        'AAPL': np.array([[day_0, feature_1, ..., feature_25],
                         [day_1, feature_1, ..., feature_25],
                         ...
                         [day_N, feature_1, ..., feature_25]]),
        'MSFT': np.array(...),
        # ... for each stock
    },
    'index_tra_dates': {0: '2012-01-03', 1: '2012-01-04', ...},
    'tra_dates_index': {'2012-01-03': 0, '2012-01-04': 1, ...}
}
```

#### 2. Data Dimensions
- **Stock array shape**: `[num_trading_days, 26]`
  - Column 0: Day index (integer)
  - Columns 1-25: Normalized stock features
- **Number of stocks**: Variable (NASDAQ has 1,026 stocks)
- **Number of trading days**: Variable (NASDAQ has 1,246 days)

#### 3. Feature Descriptions (Columns 1-25)
```
Feature Indices:
 1-5:   OHLCV (Open, High, Low, Close, Volume) - normalized
 6-10:  5-day moving averages of OHLCV
 11-15: 10-day moving averages of OHLCV  
 16-20: 20-day moving averages of OHLCV
 21-25: 30-day moving averages of OHLCV
```

#### 4. Data Preprocessing Requirements
- **Normalization**: All features must be normalized (z-score: (x - mean) / std)
- **Missing data**: Marked with `-1234` (will be excluded from inference)
- **Temporal order**: Data must be chronologically ordered
- **Sequence length**: Model uses last 32 days for prediction (configurable)

## Model Input Specifications

### Input Tensor Shape
- **Expected shape**: `[num_stocks, sequence_length, num_features]`
- **Default values**: `[N, 32, 25]` where N = number of stocks
- **Data type**: `torch.FloatTensor`
- **Device**: CUDA if available, otherwise CPU

### Feature Selection Options

#### Option 1: All Features (Default)
```python
feature_describe = 'all'  # Uses all 25 features (columns 1-25)
```

#### Option 2: Close-Only Features
```python
feature_describe = 'close_only'  # Uses columns [15, 16, 17, 18, 19]
```

## Model Output Specifications

### Raw Predictions
- **Shape**: `[num_stocks]`
- **Data type**: `torch.FloatTensor`
- **Value range**: Normalized returns (typically -3.0 to +3.0)
- **Interpretation**: Higher values indicate better expected performance

### Portfolio Selection Output
The model provides top-K stock selections with the following format:

```python
portfolio_selections = {
    'top_1': {
        'top_k': 1,
        'selected_stocks': [stock_index],
        'predictions': [prediction_value],
        'symbols': ['STOCK_SYMBOL'],
        'weights': [1.0]
    },
    'top_5': {
        'top_k': 5,
        'selected_stocks': [idx1, idx2, idx3, idx4, idx5],
        'predictions': [pred1, pred2, pred3, pred4, pred5],
        'symbols': ['STOCK1', 'STOCK2', 'STOCK3', 'STOCK4', 'STOCK5'],
        'weights': [w1, w2, w3, w4, w5]  # Sum to 1.0
    },
    # ... for each K value
}
```

### Weight Calculation
Portfolio weights are calculated proportionally to prediction scores:
```python
weight_i = max(prediction_i, 0) / sum(max(prediction_j, 0) for all j)
```

## Complete Usage Examples

### Example 1: Basic Inference
```python
from stock_inference import StockSelectionInference

# Initialize inference pipeline
inference = StockSelectionInference("./models/trained_model.ckpt")

# Run inference
results = inference.run_inference("./data/NASDAQ_all_features.pkl")

# Access results
top_10_stocks = results['portfolio_selections']['top_10']['symbols']
print(f"Top 10 stocks: {top_10_stocks}")
```

### Example 2: Custom Configuration
```python
# Custom model configuration
config = {
    'input_size': 25,
    'hidden_size': 128,
    'num_heads': 4,
    'days': 16,  # Use 16-day sequences instead of 32
    'market_name': 'NASDAQ'
}

inference = StockSelectionInference("./models/trained_model.ckpt", config)
results = inference.run_inference(
    data_source="./data/NASDAQ_all_features.pkl",
    top_k=[5, 10, 20],
    sequence_length=16,
    feature_describe='all'
)
```

### Example 3: Step-by-Step Processing
```python
inference = StockSelectionInference("./models/trained_model.ckpt")

# Step 1: Preprocess data
input_tensor, valid_mask, symbols = inference.preprocess_data(
    "./data/NASDAQ_all_features.pkl",
    sequence_length=32,
    feature_describe='all'
)

# Step 2: Run predictions
predictions = inference.predict(input_tensor, valid_mask)

# Step 3: Select top stocks
portfolios = inference.select_top_stocks(
    predictions, valid_mask, symbols, top_k=[5, 10]
)

print(f"Top 5 stocks: {portfolios['top_5']['symbols']}")
```

## Command Line Interface

### Available Arguments
```bash
python stock_inference.py \
    --model_path "./models/model_tt_50.ckpt" \          # Required: Model checkpoint
    --data_path "./data/NASDAQ_all_features.pkl" \      # Required: Input data
    --top_k 1 5 10 20 \                                 # Optional: K values
    --sequence_length 32 \                               # Optional: Sequence length
    --feature_describe all \                             # Optional: Feature set
    --output_file results.json                           # Optional: Save results
```

### Output Format
The command line interface prints structured results:
```
ðŸ“Š STOCK SELECTION INFERENCE RESULTS
================================================================================
ðŸ“… Timestamp: 2025-10-03T10:30:00.123456
ðŸ“ˆ Market: NASDAQ
ðŸ”¢ Total stocks analyzed: 1026
âœ… Valid stocks: 1023
ðŸ“Š Sequence length: 32 days
ðŸ·ï¸  Feature set: all

ðŸ† TOP 5 STOCK SELECTION:
----------------------------------------
   1. AAPL   | Prediction:   2.1456 | Weight: 32.15%
   2. MSFT   | Prediction:   1.9832 | Weight: 29.73%
   3. GOOGL  | Prediction:   1.5443 | Weight: 23.13%
   4. NVDA   | Prediction:   1.0021 | Weight: 15.00%
   5. TSLA   | Prediction:   0.8901 | Weight: 13.34%

ðŸ“Š PREDICTION STATISTICS:
   Mean prediction:   0.0234
   Std prediction:    0.9876
   Min prediction:   -2.3456
   Max prediction:    2.1456
```

## Error Handling and Troubleshooting

### Common Issues

#### 1. Model Loading Errors
```python
# Error: Model architecture mismatch
# Solution: Ensure config matches training configuration
config = {
    'input_size': 25,      # Must match training
    'hidden_size': 128,    # Must match training
    'num_heads': 4,        # Must match training
    # ... other parameters
}
```

#### 2. Data Format Errors
```python
# Error: Incorrect data shape
# Expected: [num_stocks, sequence_length, num_features]
# Common fix: Remove day index column and select correct features
data = stock_data[:, 1:]  # Remove day index (column 0)
```

#### 3. Missing Data Handling
```python
# Stocks with missing data (marked as -1234) are automatically excluded
# Check valid_mask to see which stocks are included
valid_stocks = [symbol for symbol, valid in zip(symbols, valid_mask) if valid > 0.5]
```

#### 4. CUDA Memory Issues
```python
# If CUDA out of memory, force CPU usage
inference = StockSelectionInference(model_path, config)
inference.device = torch.device('cpu')
inference.model = inference.model.cpu()
```

## Performance Expectations

### Typical Runtime
- **Data preprocessing**: 1-2 seconds for 1,026 stocks
- **Model inference**: 0.1-0.5 seconds (GPU) / 1-3 seconds (CPU)
- **Portfolio selection**: <0.1 seconds
- **Total time**: 2-6 seconds for complete pipeline

### Memory Requirements
- **Model size**: ~2-5 MB for parameters
- **Data size**: ~250 MB for NASDAQ dataset
- **Peak memory**: ~1-2 GB during inference

### Expected Accuracy
Model performance varies by market conditions:
- **Top-1 accuracy**: Typically 15-25% for daily predictions
- **Top-10 portfolio**: Often outperforms market baseline
- **Risk-adjusted returns**: Sharpe ratio typically 0.8-1.5

## Integration with Trading Systems

### Real-time Usage Pattern
```python
# Daily inference workflow
def daily_stock_selection():
    # 1. Update data with latest market information
    update_market_data()
    
    # 2. Run inference
    inference = StockSelectionInference("./models/latest_model.ckpt")
    results = inference.run_inference("./data/latest_data.pkl")
    
    # 3. Extract portfolio
    portfolio = results['portfolio_selections']['top_10']
    
    # 4. Execute trades
    execute_portfolio_trades(portfolio)
    
    return portfolio
```

### Risk Management Integration
```python
def risk_adjusted_selection(results, max_position_size=0.1):
    """Apply position sizing and risk limits."""
    portfolio = results['portfolio_selections']['top_10']
    
    # Apply maximum position size constraint
    for i, weight in enumerate(portfolio['weights']):
        portfolio['weights'][i] = min(weight, max_position_size)
    
    # Renormalize weights
    total_weight = sum(portfolio['weights'])
    portfolio['weights'] = [w / total_weight for w in portfolio['weights']]
    
    return portfolio
```

## Model Configuration Details

### Architecture Parameters
```python
default_config = {
    'input_size': 25,           # Number of features per time step
    'num_class': 1,             # Regression output (single value)
    'hidden_size': 128,         # Transformer hidden dimension
    'num_feat_att_layers': 1,   # Feature attention layers
    'num_pre_att_layers': 1,    # Prediction attention layers
    'num_heads': 4,             # Multi-head attention heads
    'days': 32,                 # Input sequence length
    'dropout': 0.1,             # Dropout rate for regularization
    'market_name': 'NASDAQ',    # Market identifier
    'feature_describe': 'all'   # Feature set selection
}
```

### Training Parameters (Reference)
```python
# These were used during training - for reference only
training_config = {
    'pretrain_epoch': 50,       # Pre-training epochs
    'epoch': 100,               # Fine-tuning epochs
    'batch_size': 64,           # Training batch size
    'learning_rate': 0.001,     # Adam optimizer learning rate
    'weight_decay': 0.01,       # L2 regularization
    'patience': 10,             # Early stopping patience
    'train_start_date': '2012-01-03',
    'train_end_date': '2016-01-01',
    'test_start_date': '2016-01-01',
    'test_end_date': '2017-01-01'
}
```

This inference system provides a complete solution for deploying the trained stock selection model in production environments with precise data processing requirements and expected output formats.